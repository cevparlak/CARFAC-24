{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cevparlak/CARFAC-24/blob/main/carfac24_cnn_lstm_cc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYxmnaKm4Y2n",
        "outputId": "7a62d220-0a57-455f-89c4-61a3f9d15813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "# !pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI_3YD8lMSVb"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "print(sys.version)\n",
        "!pip install -qq -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tih_83o6qI1m"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "## Package\n",
        "import tensorflow as tf\n",
        "\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "# import librosa.display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "from matplotlib.collections import QuadMesh\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "import plotly.tools as tls\n",
        "import seaborn as sn\n",
        "import scipy.io.wavfile\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Reshape, Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, GRU, Bidirectional\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,TimeDistributed\n",
        "\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers as L\n",
        "\n",
        "# from keras_self_attention import SeqSelfAttention\n",
        "# from attention import Attention\n",
        "\n",
        "import scipy.io as sio\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from scipy.io import arff\n",
        "## Python\n",
        "import random as rn\n",
        "import sys\n",
        "from sklearn import preprocessing\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# imports\n",
        "# import os\n",
        "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n",
        "# import my_models\n",
        "\n",
        "model_list1=['m_nv','nv','sVGG','VGG16','VGG19','Res v2','Res v1','CNN_LSTM','CNN_LSTM2','CNN_LSTM_Att']\n",
        "alist  = pd.DataFrame(columns=['Model','Train', 'Test', 'Acc', 'Loss','Precision','Recall','F1','Kappa','B_Accuracy'])\n",
        "alist2 = pd.DataFrame(columns=['Model','Train', 'Test', 'Acc', 'Loss'])\n",
        "alist3 = pd.DataFrame(columns=['Model','Train', 'Test', 'Acc', 'Loss'])\n",
        "best_acc=0\n",
        "best_loss=1000000000;\n",
        "\n",
        "\n",
        "nrows=44  # number of rows of feature vectors, columns are auto-adjusted\n",
        "batch_size1 = 16\n",
        "epoch1 = 50\n",
        "val_size1=0.2\n",
        "test_size1=0.30\n",
        "hidden_units1=512\n",
        "hidden_units2=256\n",
        "lr1=0.001;\n",
        "factor1=0.90\n",
        "patience1=10\n",
        "acf='relu'\n",
        "n1=12 # for resnet\n",
        "modelstr1='cnn_'+str(lr1)+'_'+str(factor1)+'_'+str(patience1)+'_'+str(nrows)+'_'+str(epoch1)+'_'+str(batch_size1)+'_'+acf\n",
        "\n",
        "i=1\n",
        "# np.random.seed(1234)\n",
        "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1 )\n",
        "# sess = tf.compat.v1.Session( graph=tf.compat.v1.get_default_graph(), config=session_conf )\n",
        "# tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "#-----------------------------Keras reproducibility------------------#\n",
        "# SEED = 1234\n",
        "\n",
        "# tf.set_random_seed(SEED)\n",
        "# os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# rn.seed(SEED)\n",
        "\n",
        "# session_conf = tf.ConfigProto(\n",
        "#     intra_op_parallelism_threads=1,\n",
        "#     inter_op_parallelism_threads=1\n",
        "# )\n",
        "# sess = tf.Session(\n",
        "#     graph=tf.get_default_graph(),\n",
        "#     config=session_conf\n",
        "# )\n",
        "# K.set_session(sess)\n",
        "#-----------------------------------------------------------------#\n",
        "list1=glob.glob(\"E:\\\\Research\\\\carfac\\\\data\\\\*.npy\")\n",
        "path1=\"E:\\\\Research\\\\carfac\\\\data\\\\\" # data files directory in local computer\n",
        "path1=\"/content/gdrive/My Drive/carfac/a/\"  # data files directory in GDrive\n",
        "\n",
        "list1=[\n",
        "'1  2  4  5  7  8  9_3  6_nemo train carfac_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'1  2  4  5  7  8  9_3  6_nemo train mel_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'1  2  4  5  7  8  9_3  6_nemo train mfcc_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'63  61  34  20   6  53  40  39   0  27  35   5_ased train carfac_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'63  61  34  20   6  53  40  39   0  27  35   5_ased train mel_emp3 _d __cons_512_256_0.3_f0.npy',\n",
        "'63  61  34  20   6  53  40  39   0  27  35   5_ased train mfcc_emp3 _d __cons_512_256_0.3_f0.npy',\n",
        "\n",
        "]\n",
        "\n",
        "list2=[\n",
        "'1  2  4  5  7  8  9_3  6_nemo test carfac_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'1  2  4  5  7  8  9_3  6_nemo test mel_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'1  2  4  5  7  8  9_3  6_nemo test mfcc_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'63  61  34  20   6  53  40  39   0  27  35   5_ased test carfac_emp3 _d __cons_512_256_0.3__f0.npy',\n",
        "'63  61  34  20   6  53  40  39   0  27  35   5_ased test mel_emp3 _d __cons_512_256_0.3_f0.npy',\n",
        "'63  61  34  20   6  53  40  39   0  27  35   5_ased test mfcc_emp3 _d __cons_512_256_0.3_f0.npy',\n",
        "\n",
        "]\n",
        "\n",
        "z1=0\n",
        "model_list1=(6) #,0.01,0.1)\n",
        "for cm3 in range(1):\n",
        "    # cm=model_list1[cm3]\n",
        "    cm=5\n",
        "    batch_list1=(4,8,16,32)\n",
        "    for cm2 in range(1):\n",
        "        batch_size1=batch_list1[cm2]\n",
        "        lr_list1=(0.0001,0.001)\n",
        "        for cm1 in range(2):\n",
        "            lr1=lr_list1[cm1]\n",
        "            # lr1=0.001\n",
        "\n",
        "            for st in list1:\n",
        "        #    df=arff.loadarff(st)\n",
        "                print(path1+st)\n",
        "                index1 = list1.index(st)\n",
        "                test_file=list2[index1]\n",
        "                fname1=list1[index1]\n",
        "                print(index1)\n",
        "                print(test_file)\n",
        "\n",
        "                x=np.load(path1+st)\n",
        "                y=x[:,-1]\n",
        "        #    break\n",
        "                x=x[:,:-1]\n",
        "        #    break\n",
        "                x[np.isnan(x)] = 0\n",
        "\n",
        "                #    x[~np.all(x == 0, axis=1)]\n",
        "            #    xx=np.any(np.isnan(x))\n",
        "            # normalize each column independently between [0,1]\n",
        "\n",
        "                min_max_scaler = preprocessing.MinMaxScaler()\n",
        "                x= min_max_scaler.fit_transform(x)\n",
        "                # xx=x\n",
        "                # del data\n",
        "                # feature count must be greater than 40 or so for the cnn\n",
        "                a1=x.shape[1]\n",
        "                a2=a1 % nrows\n",
        "                if a2!=0:\n",
        "                    a2=nrows-a2\n",
        "                    c1=np.zeros((x.shape[0],a2),dtype=int)\n",
        "                    x=np.concatenate((x,c1),axis=1)\n",
        "                values, counts = np.unique(y, return_counts=True)\n",
        "                n_classes=len(counts)\n",
        "                ncols=int(x.shape[1]/nrows)\n",
        "                x_train =x\n",
        "                y_train=y\n",
        "                # xxx=x_train\n",
        "                del x\n",
        "                del y\n",
        "\n",
        "                print('x_train shape:', x_train.shape)\n",
        "                print(x_train.shape[0], 'train samples')\n",
        "\n",
        "                x_train, x_val, y_train, y_val  = sklearn.model_selection.train_test_split(x_train, y_train, test_size=val_size1, random_state=1)\n",
        "\n",
        "                # input image dimensions\n",
        "                img_rows, img_cols = nrows, ncols\n",
        "\n",
        "                if K.image_data_format() == 'channels_first':\n",
        "                    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols,1)\n",
        "                    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols,1)\n",
        "                    input_shape = (1, img_rows, img_cols)\n",
        "                else:\n",
        "                    x_train = x_train.reshape(x_train.shape[0], 1,img_rows, img_cols, 1)\n",
        "                    x_val = x_val.reshape(x_val.shape[0], 1,img_rows, img_cols, 1)\n",
        "                    input_shape = (1,img_rows, img_cols, 1)\n",
        "\n",
        "\n",
        "                # convert class vectors to binary class matrices\n",
        "                y_val=y_val-1\n",
        "                y_val = keras.utils.to_categorical(y_val, n_classes)\n",
        "\n",
        "                y_train = y_train-1\n",
        "                y_trainx=y_train\n",
        "                y_train = keras.utils.to_categorical(y_train, n_classes)\n",
        "        ###################################\n",
        "        ###################################\n",
        "        ###################################\n",
        "                x=np.load(path1+test_file)\n",
        "                y=x[:,-1]\n",
        "        #    break\n",
        "                x=x[:,:-1]\n",
        "        #    break\n",
        "                x[np.isnan(x)] = 0\n",
        "\n",
        "                #    x[~np.all(x == 0, axis=1)]\n",
        "            #    xx=np.any(np.isnan(x))\n",
        "            # normalize each column independently between [0,1]\n",
        "\n",
        "                min_max_scaler = preprocessing.MinMaxScaler()\n",
        "                x= min_max_scaler.fit_transform(x)\n",
        "                # xx=x\n",
        "                # del data\n",
        "                # feature count must be greater than 40 or so for the cnn\n",
        "                a1=x.shape[1]\n",
        "                a2=a1 % nrows\n",
        "                if a2!=0:\n",
        "                    a2=nrows-a2\n",
        "                    c1=np.zeros((x.shape[0],a2),dtype=int)\n",
        "                    x=np.concatenate((x,c1),axis=1)\n",
        "                values, counts = np.unique(y, return_counts=True)\n",
        "                n_classes=len(counts)\n",
        "                ncols=int(x.shape[1]/nrows)\n",
        "                x_test=x\n",
        "                y_test=y\n",
        "            #    xxx=x_train\n",
        "                del x\n",
        "                del y\n",
        "\n",
        "                print('x_test shape:', x_test.shape)\n",
        "                print('y_test shape:', y_test.shape)\n",
        "\n",
        "                # input image dimensions\n",
        "                img_rows, img_cols = nrows, ncols\n",
        "\n",
        "\n",
        "                if K.image_data_format() == 'channels_first':\n",
        "                    x_test = x_test.reshape(x_test.shape[0], 1, img_rows,img_cols,1)\n",
        "                    input_shape = (1, img_rows,img_cols,1)\n",
        "                else:\n",
        "                    x_test = x_test.reshape(x_test.shape[0], 1,img_rows,img_cols, 1)\n",
        "                    input_shape = (1,img_rows,img_cols, 1)\n",
        "\n",
        "\n",
        "                # convert class vectors to binary class matrices\n",
        "                y_test = y_test-1\n",
        "                y_testx=y_test\n",
        "                y_test = keras.utils.to_categorical(y_test, n_classes)\n",
        "\n",
        "                # np.save(\"x_train25.npy\",x_train)\n",
        "                # np.save(\"x_test25.npy\",x_test)\n",
        "                # np.save(\"y_train25.npy\",y_train)\n",
        "                # np.save(\"y_test25.npy\",y_test)\n",
        "\n",
        "                cm=20\n",
        "                # if cm==0:\n",
        "                #   model, str1 = my_models.mini_nvidia_model(input_shape, n_classes, acf)\n",
        "                # if cm==1:\n",
        "                #   model, str1 = my_models.nvidia_model(input_shape, n_classes, acf)\n",
        "                # if cm==2:\n",
        "                #   model, str1 = my_models.sVGG(input_shape, n_classes, acf)\n",
        "                # if cm==3:\n",
        "                #   model, str1 = my_models.VGG16(input_shape, n_classes, acf)\n",
        "                # if cm==4:\n",
        "                #   model, str1 = my_models.VGG19(input_shape, n_classes, acf)\n",
        "                # if cm==5:\n",
        "                #   n1=3\n",
        "                #   model, str1 = my_models.resnet_v2(input_shape, n_classes, acf, n1)\n",
        "                # if cm==6:\n",
        "                #   n1=5\n",
        "                #   model, str1 = my_models.resnet_v1(input_shape, n_classes, acf, n1)\n",
        "                str1='svgg_lstm_seq_self_att'\n",
        "                chanDim=-1\n",
        "                str1='cnn64-128-256lstm512-256-128dnn512'\n",
        "                inp = Input(shape=(1, img_rows, img_cols, 1))\n",
        "                x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(inp)\n",
        "                x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)\n",
        "                x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n",
        "\n",
        "                x = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(inp)\n",
        "                x = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(x)\n",
        "                x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n",
        "\n",
        "                x = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(inp)\n",
        "                x = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(x)\n",
        "                x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n",
        "                x = TimeDistributed(Flatten())(x)\n",
        "                x = LSTM(units=512, return_sequences=True)(x)\n",
        "                x = LSTM(units=256, return_sequences=True)(x)\n",
        "                x = LSTM(units=128, return_sequences=False)(x)\n",
        "                x = Dropout(0.25)(x)\n",
        "                x = Flatten()(x)\n",
        "                x = Dense(512, activation='relu')(x)\n",
        "                x = Dropout(0.5)(x)\n",
        "                # x = Dense(512, activation='relu')(x)\n",
        "                # x = Dropout(0.5)(x)\n",
        "                x = Dense(n_classes, activation='softmax')(x)\n",
        "                model = Model(inp, x)\n",
        "                # model.summary()\n",
        "\n",
        "\n",
        "                # x = L.Conv2D(64, (3, 3), activation=acf, padding='same')(x)\n",
        "                # # x = L.BatchNormalization()(x)\n",
        "                # x = L.Conv2D(64, (3, 3), activation=acf, padding='same')(x)\n",
        "                # # x = L.BatchNormalization()(x)\n",
        "                # x = L.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "                # x = L.Dropout(0.25)(x)\n",
        "\n",
        "                # x = L.Conv2D(128, (3, 3), activation=acf, padding='same')(x)\n",
        "                # # x = L.BatchNormalization()(x)\n",
        "                # x = L.Conv2D(128, (3, 3), activation=acf, padding='same')(x)\n",
        "                # # x = L.BatchNormalization()(x)\n",
        "                # x = L.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "                # x = L.Dropout(0.25)(x)\n",
        "\n",
        "                # # x = L.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)\n",
        "                # x = Reshape((-1, 128))(x)\n",
        "                # print(x.shape)\n",
        "                # print('xx')\n",
        "                # # x = L.Bidirectional(LSTM(256, return_sequences=True))(x)  # [b_s, seq_len, vec_dim]\n",
        "                # # x = SeqSelfAttention(attention_activation ='tanh')(x)\n",
        "                # # x = L.Bidirectional(LSTM(128, return_sequences=False))(x)  # [b_s, seq_len, vec_dim]\n",
        "\n",
        "                # # x = L.Bidirectional(LSTM(256, return_sequences=True))(x)  # [b_s, seq_len, vec_dim]\n",
        "                # # x = L.Bidirectional(LSTM(128, return_sequences=True))(x)  # [b_s, seq_len, vec_dim]\n",
        "\n",
        "                # x = L.LSTM(512, return_sequences=True)(x)  # [b_s, seq_len, vec_dim]\n",
        "                # x = L.LSTM(256, return_sequences=True)(x)  # [b_s, seq_len, vec_dim]\n",
        "\n",
        "                # x = Attention(units=64)(x)\n",
        "                # x = L.Dense(512, activation=acf)(x)\n",
        "                # x = Dropout(0.5)(x)\n",
        "\n",
        "                # x = L.Dense(n_classes, activation='softmax', name='output')(x)\n",
        "                # model = Model(inp, x)\n",
        "\n",
        "                # if cm==6:\n",
        "                #   model, str1 = my_models.ConvSpeechModel(input_shape, n_classes, acf)\n",
        "                # if cm==7:\n",
        "                #     model, str1 = my_models.RNNSpeechModel(input_shape, n_classes, acf)\n",
        "                # if cm==8:\n",
        "                #     model, str1 = my_models.attRNNSpeechModel(input_shape, n_classes, acf)\n",
        "\n",
        "                # optimizer1 = keras.optimizers.SGD(lr=lr1, momentum=0.0, decay=0.0, nesterov=True)\n",
        "                # lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "                # lr_reduce2 = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "                # lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=factor1, patience=patience1, min_lr=0.000000001, verbose=1)\n",
        "                # callbacks = [checkpoint, lr_reduce, lr_scheduler]\n",
        "                # callbacks = [lr_reduce]\n",
        "                # opt_name='SGD';\n",
        "\n",
        "                opt_name='Adam';\n",
        "                optimizer1=keras.optimizers.Adam(learning_rate=lr1, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "                model.compile(loss='categorical_crossentropy', optimizer=optimizer1, metrics=['accuracy'])\n",
        "                head1, tail1 = os.path.split(path1+fname1)\n",
        "                modelstr1=str1+'_'+str(lr1)+'_'+str(factor1)+'_'+str(patience1)+'_'+str(nrows)+'_'+str(epoch1)+'_'+str(batch_size1)+'_'+acf+'_'+str(test_size1)\n",
        "                # modelstr2=modelstr1+'_'+opt_name\n",
        "                modelstr2=modelstr1+'_'+opt_name+'_CC'\n",
        "\n",
        "                rlrop1 = ReduceLROnPlateau(monitor='loss', factor=factor1, patience=patience1, min_lr=0.000000001, verbose=1)\n",
        "\n",
        "                history=model.fit(x_train, y_train,\n",
        "                              epochs=epoch1,\n",
        "                              validation_data=(x_val, y_val),\n",
        "                              callbacks=[rlrop1]\n",
        "                              )\n",
        "                score = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "                #Predict\n",
        "                print('Test accuracy:', score[1])\n",
        "                print('Test loss:', score[0])\n",
        "                # head1, tail1 = os.path.split(path1+st)\n",
        "\n",
        "            # Plotting the Train Valid Loss Graph\n",
        "                r1=8\n",
        "                c1=6\n",
        "                f1=16\n",
        "                linew1=3\n",
        "                plt.clf()\n",
        "                plt.figure(figsize=(r1,c1))\n",
        "                plt.rcParams.update({'font.size': f1})\n",
        "                plt.plot(history.history['loss'],linewidth=linew1)\n",
        "                plt.plot(history.history['val_loss'],linewidth=linew1)\n",
        "                plt.title('Model Loss')\n",
        "                plt.xlabel('epoch')\n",
        "                plt.ylabel('loss')\n",
        "                plt.legend(['train', 'test'])\n",
        "            #    plt.legend(['training', 'test'], loc='upper left')\n",
        "            #    plt.show()\n",
        "                plt.savefig(head1+'/b/aloss_'+tail1+'_'+modelstr2+'.png')\n",
        "                # np.save(head1+'\\\\aloss_'+tail1+'_'+modelstr2+'.npy', history.history['loss'])\n",
        "                # np.save(head1+'\\\\aloss_val_'+tail1+'_'+modelstr2+'.npy', history.history['val_loss'])\n",
        "\n",
        "                plt.clf()\n",
        "                plt.figure(figsize=(r1,c1))\n",
        "                plt.rcParams.update({'font.size': f1})\n",
        "                plt.plot(history.history['accuracy'],linewidth=linew1)\n",
        "                plt.plot(history.history['val_accuracy'],linewidth=linew1)\n",
        "                plt.title('Model Accuracy')\n",
        "                plt.xlabel('epoch')\n",
        "                plt.ylabel('accuracy')\n",
        "                plt.legend(['train','test'])\n",
        "            #    plt.show()\n",
        "                plt.savefig(head1+'/b/acc_'+tail1+'_'+modelstr2+'.png')\n",
        "                # np.save(head1+'\\\\acc_'+tail1+'_'+modelstr2+'.npy', history.history['accuracy'])\n",
        "                # np.save(head1+'\\\\acc_val_'+tail1+'_'+modelstr2+'.npy', history.history['val_accuracy'])\n",
        "            #    plt.clf()\n",
        "                print(head1+'/aloss_'+tail1+'_'+modelstr2+'.png')\n",
        "\n",
        "                # y_pred = model.predict(x_test, batch_size = batch_size1)\n",
        "                y_pred = model.predict(x_test)\n",
        "                y_test1= np.argmax(y_test, axis=1)\n",
        "            #    y_test1=y_test1.tolist()\n",
        "                y_pred1 = np.argmax(y_pred, axis=1)\n",
        "            #    y_pred1 = y_pred1.tolist()\n",
        "                # predict probabilities for test set\n",
        "                yhat_probs = y_pred\n",
        "                # predict crisp classes for test set\n",
        "                # yhat_classes = model.predict_classes(x_test, verbose=0)\n",
        "                predict_x=model.predict(x_test)\n",
        "                yhat_classes=np.argmax(predict_x,axis=1)\n",
        "\n",
        "                # reduce to 1d array\n",
        "                yhat_probs = yhat_probs[:, 0]\n",
        "            #    yhat_classes = yhat_classes[:, 0]\n",
        "                # accuracy: (tp + tn) / (p + n)\n",
        "                accuracy = accuracy_score(y_test1, yhat_classes)\n",
        "                print('Accuracy: %f' % accuracy)\n",
        "                # balanced accuracy: (tp + tn) / (p + n)\n",
        "                b_accuracy = balanced_accuracy_score(y_test1, yhat_classes)\n",
        "                print('Balanced Accuracy: %f' % b_accuracy)\n",
        "                # precision tp / (tp + fp)\n",
        "                precision = precision_score(y_test1, yhat_classes,average='weighted')\n",
        "                print('Precision: %f' % precision)\n",
        "                # recall: tp / (tp + fn)\n",
        "                recall = recall_score(y_test1, yhat_classes,average='weighted')\n",
        "                print('Recall: %f' % recall)\n",
        "                # f1: 2 tp / (2 tp + fp + fn)\n",
        "                f1 = f1_score(y_test1, yhat_classes,average='weighted')\n",
        "                print('F1 score: %f' % f1)\n",
        "                # kappa\n",
        "                kappa = cohen_kappa_score(y_test1, yhat_classes)\n",
        "                print('Cohens kappa: %f' % kappa)\n",
        "            #    ROC AUC\n",
        "            #    auc = roc_auc_score(y_test1, yhat_probs)\n",
        "            #    print('ROC AUC: %f' % auc)\n",
        "            #    confusion matrix\n",
        "                conf_mat = confusion_matrix(y_test1, yhat_classes)\n",
        "                df=pd.DataFrame(data=conf_mat[0:,0:], index=[i for i in range(conf_mat.shape[0])], columns=['f'+str(i) for i in range(conf_mat.shape[1])])\n",
        "                df.to_excel(head1+'/b/aconf_mat_'+tail1+'_'+modelstr2+'.xlsx')\n",
        "                print(conf_mat)\n",
        "            #        np.savetxt(head1+'\\\\conf_mat_'+tail1+'_'+modelstr1+'.csv', conf_mat, '%s', delimiter=\",\")\n",
        "                alist.loc[i]=[str1,tail1+'_'+modelstr2,'',score[1],score[0],precision,recall,f1,kappa,b_accuracy]\n",
        "                i=i+1\n",
        "            #    np.savetxt(head1+'\\\\alist_'+'_'+modelstr1+'.csv', alist, '%s', delimiter=\"/t\")\n",
        "                del x_train\n",
        "                del x_val\n",
        "                del x_test\n",
        "                best_acc3=0\n",
        "                best_acc4=0\n",
        "                z1=z1+1\n",
        "                if (z1%6==0):\n",
        "                    alist.to_excel(head1+'/b/alist_'+'_'+tail1+'_'+modelstr2+'.xlsx')\n",
        "\n",
        "            alist.to_excel(head1+'/b/alist_'+'_'+tail1+'_'+modelstr2+'.xlsx')"
      ],
      "metadata": {
        "id": "LgXTiqYxv5al",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "d111825b-e57f-4e69-82b6-706589fad194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/carfac/a/1  2  4  5  7  8  9_3  6_nemo train carfac_emp3 _d __cons_512_256_0.3__f0.npy\n",
            "0\n",
            "1  2  4  5  7  8  9_3  6_nemo test carfac_emp3 _d __cons_512_256_0.3__f0.npy\n",
            "x_train shape: (3685, 2904)\n",
            "3685 train samples\n",
            "x_test shape: (796, 2904)\n",
            "y_test shape: (796,)\n",
            "Epoch 1/50\n",
            "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 390ms/step - accuracy: 0.1852 - loss: 1.7822 - val_accuracy: 0.3894 - val_loss: 1.4135 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m45/93\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 361ms/step - accuracy: 0.4052 - loss: 1.3544"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPhIBjUIqlBHJoVJMbwklZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}